# Chat Configuration Notes

- **Models & token limits**: set planner/evidence/answer models in `chat.config.yml` (`models` + per-stage `tokens`); the embedding model defaults from `chat-preprocess.config.yml` (override in `chat.config.yml` if needed). The runtime fails fast if any required model is missing after those fallbacks. Recommended: planner/evidence/answer `gpt-5-nano-2025-06-01`, embedding `text-embedding-3-large`. Token ceilings in config flow into `ChatRuntimeOptions.tokenLimits`.
- **Reasoning**: Emitted only when the request sets `reasoningEnabled`; no additional visibility modes are wired from config.
- **Semantic ranking & embeddings**: Projects and resume corpora use embedding-assisted ranking in `packages/chat-next-api`. Defaults: `text-embedding-3-large` for both (or `models.embedding` in `chat.config.yml`), semantic score weights of `12` (projects) and `8` (experiences). Env overrides are removed; change these via config or code when bootstrapping chat.
- **Retrieval caps & evidence limits**: Planner can emit any nonnegative `topK`, but the runtime clamps between `3` and `10` (`defaultTopK=8`, `maxTopK=10`; `enumerateAllRelevant` bumps to `50`). Evidence selection enforces `<=10` docs per retrieval request, with per-corpus caps (projects/experiences 6 each, education/awards/skills 4 each, 12 total). `createPortfolioChatServer` exposes `retrievalOverrides` for `defaultTopK`/`maxTopK` if we need different blend sizes.
- **Debug + logging**: `CHAT_DEBUG_LOG` levels â€” `0` off, `1` readable (default in dev), `2` verbose (includes `.raw` model outputs), `3` redacted-safe (keeps text but strips secret-shaped fields like keys/tokens/auth). `CHAT_DEBUG_LOG_LIMIT` bounds the buffer, and `runWithChatLogContext` (AsyncLocalStorage) tags every event with the per-request correlation id for CloudWatch correlation.
- **Fast paths**: meta/greeting plans skip retrieval queries entirely and synthesize an evidence summary so the Answer LLM just uses persona/identity context. If retrieval returns zero docs, we also synthesize evidence, attach the `ZERO_EVIDENCE_BANNER`, and bypass the Evidence LLM to save tokens.
